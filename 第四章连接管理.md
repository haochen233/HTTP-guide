本章内容概要：  
- HTTP是如何使用 TCP 连接的；
- TCP 连接的时延、瓶颈以及存在的障碍；
- HTTP 的优化，包括并行连接、keep-alive（持久连接）和管道化连接；
- 管理连接时应该以及不应该做的事情。

# 4.1 TCP连接
## 4.1.1 TCP的可靠数据管道
TCP为HTTP提供了一条可靠的比特传输管道。从 TCP 连接一端填入的字节会从另一端以原有的顺序、正确地传送出来。

如果在浏览器输入`http://www.baidu.com/index.html`

1. 浏览器解析出主机名
2. 浏览器查询www.baidu.com域名对应的IP地址
3. 浏览器获得端口号80
4. 浏览器根据IP和端口发起到服务器TCP连接
5. 连接建立好后，浏览器向服务器发送一条HTTP GET报文
6. 然后服务器受理了该请求。向浏览器发送了响应报文。
7. 浏览器关闭TCP连接

## 4.1.2 TCP流是分段的、由IP分组传送
TCP的数据是通过名为IP分组（或IP数据报）的小数据块来发送的。

当HTTP要传送一条报文时，会以流的形式将报文数据的内容通过一条打开的TCP连接按序传输，TCP收到数据流之后，会将数据流砍成被称作段的小数据块，并将段封装在IP分组中，通过因特网传输。

每个TCP段都是由IP分组承载，从一个IP地址发送到另一个IP地址。每个IP分组都包括：  
- 一个IP分组首部（通常为20字节）
- 一个TCP段首部（通常20字节）
- 一个TCP数据块（0或多个字节）

IP分组首部包含了源和目的IP地址、长度和其他一些标记。TCP段的首部包含了TCP端口号、TCP控制标记，以及用于数据排序和完整性检查的一些数字值。

## 4.1.3 保持TCP连接持续不断地运行

## 4.1.4 用TCP套接字编程
略

# 4.2 对TCP性能的考虑
HTTP紧挨着TCP，位于其上层，所以HTTP事务的性能在很大程度上取决于底层TCP通道的性能。

## 4.2.1 HTTP事务的时延
我们来回顾一下，在 HTTP 请求的过程中会出现哪些网络时延（耗时操作）。
HTTP 事务主要的连接、传输以及处理时延：  
1. 客户端进行DNS查询
2. 客户端与服务器建立TCP连接
3. 客户端发送请求
4. 服务器处理请求然后发送响应
5. 客户端关闭连接

这些都需要花费时间

这些TCP网络时延的大小取决于硬件速度、网络和服务器的负载，请求和响应报文的尺寸，以及客户端和服务器之间的距离。

## 4.2.2 性能聚焦区域
会对HTTP产生影响的、最常见的TCP相关时延，其中包括：  
- TCP连接建立握手
- TCP慢启动拥塞控制
- 数据聚集的Nagle算法
- 用于捎带确认的TCP延迟确认算法
- TIME_WAIT时延和端口耗尽

## 4.2.3 TCP连接的握手时延
TCP连接握手需要经过一下几个步骤

1. 第一次握手：请求新的TCP连接时，客户端向服务器发送一个小的TCP分组，分组中设置了一个特殊的SYN标记，说明这是一个连接请求
2. 如果服务器接收连接，就会对一些参数进行计算，并向客户端回送一个TCP分组，这个分组的SYN和ACK标记都被置位，说明连接请求已经被接受
3.    最后，客户端向服务器回送一条确认信息，通知它连接已成功建立，而且现代的TCP栈都允许客户端在这个确认分组中发送数据。

小的HTTP事务可能会在TCP建立上花费50%，或更多时间。

## 4.2.4 延迟确认
由于因特网自身无法确保可靠的分组传输，所以TCP实现了自己的确认机制来确保数据的成功传输。

每个TCP段都有一个序列号和数据完整性校验和。每个段的接受者收到完好的段时，都会向发送者回送小的确认分组（ACK）。如果发送者没有在指定的窗口时间内收到确认信息，发送者就认为分组已被破坏或损毁，并重发数据。（也就是重发机制）

由于确认报文很小，所以TCP允许在发往相同方向输出数据，分组中对其进行“捎带”。TCP将返回给对端的确认信息（ACK）与输出的数据分组结合在一起，可以更有效地利用网络。为了增加确认报文找到**同向传输数据分组**的可能性，很多 TCP 栈都实现了一种“延迟确认”算法，延迟确认算法会在一个特定的窗口时间（通常是 100 ～ 200 毫秒）内将输出确认存放在缓冲区中，以寻找能够捎带它的输出数据分组。如果在那个时间段内没有输出数据分组，就将确认信息放在单独的分组中传送。

但是，HTTP具有双峰特征的请求——应答行为降低了捎带信息的可能。通常，延迟确认算法会引入相当大的时延。可以调整或禁止延迟确认算法。

> 在对 TCP 栈的任何参数进行修改之前，一定要对自己在做什么有清醒的认识。TCP中引入这些算法的目的是防止设计欠佳的应用程序对因特网造成破坏。对 TCP 配置进行的任意修改，都要绝对确保应用程序不会引发这些算法所要避免的问题。

## 4.2.5 TCP慢启动
TCP数据传输的性能还取决于TCP 连接的使用期（age）。TCP连接会随着时间进行自我“调谐"起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度。这种调谐被称为 TCP 慢启动（slow  start） ，用于防止因特网的**突然过载和拥塞。**

TCP 慢启动限制了一个 TCP 端点在任意时刻可以传输的分组数。简单来说，每成功接收一个分组，发送端就有了发送另外两个分组的权限。如果某个 HTTP 事务有大量数据要发送，是不能一次将所有分组都发送出去的。必须发送一个分组，等待确认；然后可以发送两个分组，每个分组都必须被确认，这样就可以发送四个分组了，以此类推。这种方式被称为**打开拥塞窗口** 。

由于存在这种拥塞控制特性，所以新连接的传输速度会比已经交换过一定量数据的、“以调谐”连接慢一些。由于已调谐连接要更快一些，所以HTTP中有一些可以重用现存连接的工具。

## 4.2.6 Nagle算法与TCP_NODELAY
TCP有一个数据流接口，应用程序可以通过它将任意尺寸的数据放入TCP栈中————即使一次只放一个字节，但是，每个TCP段都至少还包含了40个字节的标记和首部，所以如果TCP发送大量包含少量数据的分组，网络的性能就会严重下降

> 举个例子：假如有1000个字节的数据待发送，如果将这1000个字节装入一个TCP段中发送，相当于总共发送了1000（数据大小）+40（首部和标记大小）=1040个字节的数据。但是，如果将1000个字节数据平均分配给10个TCP段来发送，总共就会发送10*（100+40）=1400字节。很明显发送数据量过于小的分组，会严重影响网络性能。

NAgle算法试图在发送一个分组之前，将大量TCP数据绑定在一起，以提高网络效率。

Nagle算法鼓励发送全尺寸（LAN 上最大尺寸的分组大约是 1500 字节，在因特网上是几百字节）的段。
1. 只有当所有其他分组都被确认之后，Nagle 算法才允许发送非全尺寸的分组
2. 如果其他分组仍然在传输过程中，就将那部分数据缓存起来。只
有当挂起分组被确认。或者缓存中积累了足够发送一个全尺寸分组的数据时，才会将缓存的数据发送出去。

Nagle 算法会引发几种 HTTP 性能问题。首先，小的 HTTP 报文可能无法填满一个分组，可能会因为等待那些永远不会到来的额外数据而产生时延。

其次，Nagle 算法与延迟确认之间的交互存在问题：
> 例如：A、B两端之间发送，B向A发送了一个1号分组，A由于延迟确认机制，A延迟200ms发送1号分组确认信息，然后B收到1号分组的确认信息，才能发送2号分组。

从上例可以Nagle和延迟确认会将低数据的实时性，降低了性能（因为开启了Nagle算法，通时只允许有一个未被确认的分组，直到确认了这个分组，才能发送下一个分组。）（NAgle会尽可能一次发送大块数据，所以才会限制同时只能有一个未确认分组。）

HTTP 应用程序常常会在自己的栈中设置参数 TCP_NODELAY（不拖延），禁用 Nagle 算法，提高性能。如果要这么做的话，一定要确保会向 TCP 写入大块的数据，这样就不会产生一堆小分组了。


## 有关多线程下载大文件的冷知识：
>多线程从网上下载一个大文件为什么要更快（比之单线程）？网上查了有人说“是因为io堵塞的原因，因为网速肯定快不过cpu，单线程单io通道，而多线程多io通道“ 。我理解他的意思是：单线程因为网速慢赶不上cpu的处理速度，所以造成大量的堵塞，而多线程多io通道，所以堵塞减少。我的疑问是：同一个多核处理器，io通道会随着线程的增加而一直增加吗？单线的一个io通道和多线程中每个io通道速度都是一样的吗？单线程为什么不能通过提升io通道的速度进而提快速度呢？难道是因为io通道是一个硬件？其速度是受限于硬件的？如果io通道是硬件的话，那么一个处理器的最大io通道个数是不是就是和其核数相等呢？如果真的是我上面猜想的，那么是不是可以认为，如果是单核处理器，通过时间片实现的多线程下载大文件并不能更快呢？

>决定用户下载大文件速度快慢的终极因素，在于用户下载进程实时抢占网络带宽的大小。其它的因素与它相比，可以忽略不计。

### 实时最大可用带宽
任意一个与互联网通信的进程，理论上都有一个实时最大可用带宽，这是实时可用带宽，这是客观存在，不以用户意志为转移。

如果：**用户进程实时抢占的带宽 = 实时网络可用带宽**  

那是最最理想的，用户进程100%利用网络带宽，无论进程（Process）是单线程（Thread）的还是多线程的，下载速度几乎没有任何区别。

理想是丰满的，但现实是骨感的，因为：

**用户进程实时抢占的带宽 ≤实时网络可用带宽     Forever！！！**

既然如此，如果能让用户进程实时抢占的带宽无限接近实时网络可用带宽，那也是非常完美的。可是，实时网络带宽是多少？

没有人知道！实时网络可用带宽每一刻都在变化！

操作系统很愿意为用户效劳，TCP通过流量探测机制，不间断地探测实时网络可用带宽，并将实时的发送速率与之匹配（相等）起来，这个骚操作看起来很美！

### 为什么这么说呢？
传统的TCP流量探测机制有一个非常致命的缺陷：一旦检测到有丢包，立马将发送速率降为1/2。降速1/2后，如果没有丢包，将会在1/2速率的基础上，按照固定的增长值（线性增长），加大发送的速率。接下来就会一直按照这个节奏到达丢包的那一刻（**实时可用带宽**）为止。然后再1/2降速，循环往复，直到文件下载结束。

很显然，**指数级降速，线性增速**，这很不公平！降速很快，但升速却很漫长！造成的直接恶果就是真实的传输速率远远小于实时可用带宽。

### 多线程Vs 单线程
多线程相比单线程的优势是，由于有多个线程在竞争实时可用带宽。尽管多线程逻辑上是并行的，但其实还是按时序的串行处理。所以每个线程处于的阶段并不一致。

在任意时刻，有的线程处于丢包被罚1/2降速，有的线程处于2倍增速阶段（SlowStart），而有的线程处于线性增长阶段。通过多个线程的下载速率的加权平均，得到的是一根相对平滑的下载曲线。这条平滑曲线在大多数时候应该位于单线程下载速率的上方。这就是多线程下载速率更有优势的体现。

但是，如果TCP流量探测机制更加智能，比如BBR算法。BBR算法最大的进步，就是摒弃传统TCP流量调度算法（基于是否丢包而升速或降速）， BBR采取的是，实时测量网络最大的可用带宽，并将发送速率与之相匹配，一直在实时可用带宽附近小范围徘徊，避免大起大落的情况发生。测量速率能无限接近实时可用带宽，多线程相比单线程，优势就体现不出来了。

## 4.2.7 TIME_WAIT累积与端口耗尽
TIME_WAIT 端口耗尽是很严重的性能问题，会影响到性能基准，但在现实中相对较少出现，。大多数遇到性能基准问题的人最终都会碰到这个问题，而且性能都会变得出乎意料地差，所以这个问题值得特别关注。

当某个TCP端点关闭TCP连接时，会在内存中维护一个小的控制块用来记录最近所关闭连接的 IP 地址和端口号。这类信息只会维持一小段时间（大概两分钟），以确保在这段时间内不会长久具有相同地址和端口号的新连接。


>客户端每次连接到服务器上面去时都会获得一个新的源端口，以实现连接的唯一性。但由于可用源端口的数量有限（比如，60  000 个） ，而且在 2MSL 秒（比如，120秒）内连接是无法重用的，连接率就被限制在了 60 000/120=500 次 / 秒。就可确保不会遇到 TIME_WAIT 端口耗尽问题。要修正这个问题，可以增加客户端负载生成机器的数量(负载服务器)即使没有遇到端口耗尽问题，也要特别小心有大量连接处于打开状态的情况，或为处于等待状态的连接分配了大量控制块的情况。在有大量打开连接或控制块的情况下，有些操作系统的速度会严重减缓。

>不管服务器端还是客户端都会有TIME_WAIT状态（先发送FIN消息的会进入TIME_WAIT状态）

TIME_WAIT状态的用处：  
关闭连接发起方，会进入TIME_WAIT状态：  
1.  如果发起关闭方最后发送给被动关闭方的ACK消息丢失了，被动没有收到ACK会重新传递FIN+ACK，如若没有TIME_WAIT状态主动关闭方就不能接受到重传的FIN+ACK了，这回导致被动关闭方无法关闭（一直收不到ACK消息） 。    
2.  假如之前此连接有数据包严重延迟，而在它到来之前发送方已经重发了该报文。并且正常关闭了连接，假如没有TIME_WAIT状态，随后又建立了一个与之前相同IP、Port的连接，而之前被推迟的该报文在这时恰好到达，而此时此新连接非彼连接，从而会发生数据错乱。因此TIME_WAIT状态可以维持一段时间不能建立相同的新连接，使迟到的报文在网络中完全消失，就不会出现数据错乱。

# 4.3 HTTP连接的处理

## 4.3.1 常被误解的Connection首部
Connection 首部可以承载 3 种不同类型的标签，因此有时会很令人费解：
- HTTP 首部字段名，列出了只与此连接有关的首部；  
- 任意标签值，用于描述此连接的非标准选项； 
- 值 close，说明操作完成之后需关闭这条持久连接。

`客户端（client）->代理（proxy）-> 服务器（server）`

1. 本地首部即指的是客户端（向代理）发起http请求时所指定的header。你可以通过Connection指定需要保护（代理转发时移除这些头）的header字段。
2. Connection是一个控制头，仅用于当前连接（例如local-proxy过程）代理接收到请求后即使用Connection相关参数及移除Connection指定的头并产生新的Connection头，向目标服务器发起请求。

即如果客户端request设置了connection首部字段，那么如果第一层接收请求的是远端服务器，那么connection就没有作用了，假如是代理接收到了本地请求，并且在转发之前需要将**connection首部以及connection首部都删除掉再转发**

http首部可以分为两种：  
- 端到端首部（End-to-end）：端到端首部，代理在转发时必须携带的
- 逐跳首部（Hop-by-hop）：逐跳首部只对单次转发有效，代理在转发时，必须删除这些首部：例如Connection、Keep-Alive等

## 4.3.2 串行事务处理时延
串行可以简单的理解为按顺序一个一个执行，与并行（一次进行多个）相反。

访问一个包含了 3 个嵌入图片的 Web 页面。浏览器需要发起 4 个 HTTP 事务来显示此页面，1 个用于顶层的 HTML 页面，3 个用于嵌入的图片。如果每个事务都需要（串行地建立）一条新的连接，那么连接时延和慢启动时延就会叠加起来。

还有几种现存和新兴的方法可以提高 HTTP 的连接性能：  
- 并行连接  
通过多条TCP连接发起并发的HTTP请求。

- 持久连接  
重用TCP连接，以消除连接和关闭时延（慢启动和TIME_WAIT）

- 管道化连接  
通过共享的 TCP 连接发起并发的 HTTP 请求。

- 复用的连接  
交替传输请求和响应报文（时延阶段）

## 4.4.1 　并行连接可能会提高页面的加载速度
因为多个事务是并行的，连接延迟是重叠的。

## 4.4.2 并行连接不一定更快
即使并行连接的速度可能会更快，但并不一定总是更快。客户端的网络带宽不足的情况下一个HTTP事务就会很容器消耗尽所有的带宽。如果此时并行加载多个对象，每个对象都会去争抢着有限的带宽，这样性能甚至没有提升。

而且，打开大量连接会消耗很多内存资源，从而引发自身的性能问题。

假如一个复杂的WEb页面会有数10个或数百个内嵌对象。客户端可能可以打开数百个连接，但Web服务器通常会处理多个用户的请求。这样每个用户都开100个连接，服务器就用处理人数*100的连接。会造成服务器性能下降。

实际上，浏览器确实使用了并行连接，但它们会将并行连接的总数限制为一个较小的值（通常是 4 个） 。服务器可以随意关闭来自特定客户端的超量连接。


# 4.5 持久连接
Web 客户端经常会打开到同一个站点的连接。比如，一个 Web 页面上的大部分内嵌图片通常都来自同一个 Web 站点，而且相当一部分指向其他对象的超链通常都指向同一个站点。因此，初始化了对某服务器 HTTP 请求的应用程序很可能会在不久的将来对那台服务器发起更多的请求（比如，获取在线图片） 。这种性质被称为站点局部性（site locality）

因此HTTP/1.1 允许HTTP设备在事务处理结束之后将TCP连接保持在打开状态，以便为未来的 HTTP 请求重用现存的连接在事务处理结束之后仍然保持在打开状态的 TCP 连接被称为持久连接。非持久连接会在每个事务结束之后关闭。持久连接会在不同事务之间保持打开状态，直到客户端或服务器决定将其关闭为止

> 重用已对目标服务器打开的空闲持久连接，就可以避开缓慢的连接建立阶段。而且，已经打开的连接还可以避免慢启动的拥塞适应阶段，以便更快速地进行数据的传输。

## 4.5.1 持久以及并行连接
并行连接可以提高复合页面的传输速度。但并行连接也有一些缺点：  
- 每个事务都会打开/关闭一条新的连接，会耗费时间和带宽
- 由于 TCP 慢启动特性的存在，每条新连接的性能都会有所降低。
- 可打开的并行连接数量实际上是有限的。

持久连接有一些比并行连接更好的地方。持久连接降低了时延和连接建立的开销，将连接保持在已调谐状态，而且减少了打开连接的潜在数量。但是，管理持久连接时要特别小心，不然就会累积出大量的空闲连接，耗费本地以及远程客户端和服务器上的资源。

**持久连接与并行连接配合使用可能是最高效的方式**。现在，很多 Web 应用程序都会打开少量的并行连接，其中的每一个都是持久连接。持久连接有两种类型：  
- HTTP/1.0+的“keep-alive”连接，以及现代的HTTP/1.1 的“persistent”连接。

## 4.5.2 HTTP/1.0+ keep-alive连接
相比于串行，频繁的建立连接。keep-alive持久连接去除了连接和关闭连接的时间开销。（由于去除了慢启动拥塞控制。请求和响应时间也有可能缩减）。

## 4.5.3　Keep-Alive操作
keep-alive 已经不再使用了，而且在当前的 HTTP/1.1 规范中也没有对它的说明了。但浏览器和服务器对 keep-alive 握手的使用仍然相当广泛。

实现 HTTP/1.0  keep-alive 连接的客户端可以通过包含 Connection: Keep-Alive首部请求将一条连接保持在打开状态。

如果服务器愿意为下一条请求将连接保持在打开状态，就在响应中包含相同的首部（Connection: Keep-Alive）如果响应中没有 Connection: Keep-Alive 首部，客户端就认为服务器不支持 keep-alive。就会关闭连接。

## 4.5.4 Keep-Alive选项
注意，keep-Alive 首部只是请求将连接保持在活跃状态。发出 keep-alive 请求之后，客户端和服务器并不一定会同意进行 keep-alive 会话。它们可以在任意时刻关闭空闲的 keep-alive 连接，并可随意限制 keep-alive 连接所处理事务的数量。

可以用 `Keep-Alive` 通用首部中指定的、由逗号分隔的选项来调节 keep-alive 的行为。

- 参数timeout是在 Keep-Alive 响应首部发送的。它估计了服务器希望将连接保持在活跃状态的时间。这并不是一个承诺值。

- 参数 max 是在 Keep-Alive 响应首部发送的。它估计了服务器还希望为多少个
事务保持此连接的活跃状态。这并不是一个承诺值。

例如:  

```
Connection: Keep-Alive
Keep-Alive: max=5, timeout=120
```

上例说明服务器最多多还会为另外 5 个事务保持连接的打开状态，或者将打开状态保持到连接空闲了 2 分钟之后。

## 4.5.5 Keep-Alive连接的限制和规则
使用 keep-alive 连接时有一些限制和一些需要澄清的地方。

- 在 HTTP/1.0 中，`keep-alive`并不是默认使 用的。客户端必须发送一个
`Connection: Keep-Alive` 请求首部来激活 `keep-alive` 连接。

- `Connection:   Keep-Alive` 首部必须随所有希望保持持久连接的报文一起发送。如果客户端没有发送 Connection: Keep-Alive 首部，服务器就会在那条请求之后关闭连接。

- 通过检测响应中是否包含`Connection: Keep-Alive` 响应首部，客户端可以判断服务器是否会在发出响应之后关闭连接。

- 只有在无需检测到连接的关闭即可确定报文实体主体部分长度的情况下，才能将连接保持在打开状态——也就是说实体的主体部分必须有正确的 Content-Length。
在一条keep-alive 信道中回送错误的 Content-Length 是很糟糕的事。事务处理的另一端就无法精确地检测出一条报文的结束和另一条报文的开始了。

- 代理和网关必须执行  Connection 首部的规则。代理或网关必须在将报文转发出去或将其高速缓存之前，删除在 Connection 首部中命名的所有首部字段以及Connection 首部自身。

- 严格来说，不应该与无法确定是否支持Connection 首部的代理服务器建立keep-alive 连接。以防出现哑代理。

从技术上来讲，应该忽略所有来自 HTTP/1.0 设备的Connection 首部字段（包括 Connection: Keep-Alive） ，因为它们可能是由比较老的代理服务器误转发的。

## 4.5.6Keep-Alive和哑代理
### 1.Connection首部和盲中继
问题出在代理上——尤其是那些不理解 Connection 首部，而且不知道在沿着转发链路将其发送出去之前，应该将该首部删除的代理。很多老的或简单的代理都是盲中继（blind  relay）） ，它们只是将字节从一个连接转发到另一个连接中去，不对Connection 首部进行特殊的处理。

## 4.5.7 插入Proxy-Connection
问题是哑代理盲目地转发 Connection: Keep-Alive 之类的逐跳首部惹出了麻烦。逐跳首部只与一条特定的连接有关，不能被转发。当下游服务器误将转发来的首部作为来自代理自身的请求解释，用它来控制自己的连接时，就会引发问题。

>网景的变通做法是浏览器会向代理发送非标准的 Proxy-Connection 扩展首部，而不是官方支持的著名的 Connection 首部。如果代理是盲中继，它会将无意义的 Proxy-Connection 首部转发给 Web 服务器，服务器会忽略此首部，不会带来任何问题。但如果代理是个聪明的代理（能够理解持久连接的握手动作） ，就用一个 Connection 首部取代无意义的 Proxy-Connection 首部，然后将其发送给服务器，以收到预期的效果。

不过对有多层代理的情况，Proxy-Connection 仍然无法解决问题。

假如两层代理，第一层为哑代理，第二层为聪明代理。哑代理依旧会将Connection传回客户端，让客户端挂起。

如果两层哑代理或两层聪明代理那么不会出现问题。

## 4.5.8 HTTP/1.1持久连接
HTTP/1.1 逐渐停止了对 keep-alive 连接的支持，用一种名为持久连接（persistent connection）的改进型设计取代了它。持久连接的目的与 keep-alive 连接的目的相同，但工作机制更优一些。

HTTP/1.1 持久连接在默认情况下是激活
的。除非特别指明，否则 HTTP/1.1 假定所有连接都是持久的。要在事务处理结束之后将连接关闭，HTTP/1.1 应用程序必须向报文中显式地添加一个 Connection: close 首部。这是与以前的 HTTP 协议版本很重要的区别

HTTP/1.1 客户端假定在收到响应后，除非响应中包含了 Connection: close 首部，不然 HTTP/1.1 连接就仍维持在打开状态。但是，客户端和服务器仍然可以随时关闭空闲的连接。不发送 Connection: close 并不意味着服务器承诺永远将连接保持在打开状态。

## 4.5.9 持久连接的限制和规则
- 发送了Connection: close 请求首部之后，客户端就无法在那条连接上发送更多的请求了。

- 如果客户端不想在连接上发送其他请求了，就应该在最后一条请求中发送一个
Connection: close 请求首部。

- 只有当连接上所有的报文都有正确的、自定义报文长度时——也就是说，实体主体部分的长度都和相应的 Content-Length 一致，或者是用分块传输编码方式编码的——连接才能持久保持。

- HTTP/1.1 的代理必须能够分别管理与客户端和服务器的持久连接——每个持久连接都只适用于一跳传输。

- （由于较老的代理会转发 Connection 首部，所以）HTTP/1.1 的代理服务器不应该与 HTTP/1.0 客户端建立持久连接，除非它们了解客户端的处理能力。实际上，这一点是很难做到的，很多厂商都违背了这一原则。

- 但不管 Connection 首部取了什么值，HTTP/1.1 设备都可以在任意时刻关闭连接。

- HTTP/1.1 应用程序必须能够从异步的关闭中恢复出来。只要不存在可能会累积起来的副作用，客户端都应该重试这条请求。

- 除非重复发起请求会产生副作用，否则如果在客户端收到整条响应之前连接关闭了，客户端就必须要重新发起请求。

- 一个用户客户端对任何服务器或代理最多只能维护两条持久连接，以防服务器过载。代理可能需要更多到服务器的连接来支持并发用户的通信，所以，如果有N 个用户试图访问服务器的话，代理最多要维持 2N 条到任意服务器或父代理的 连接。 这样代理服务器和remote服务器都会过载。

# 4.6 管道化连接
HTTP/1.1 允许在持久连接上可选地使用请求管道。这是相对于 keep-alive 连接的又一性能优化。在响应到达之前，可以将多条请求放入队列。当第一条请求通过网络流向地球另一端的服务器时，第二条和第三条请求也可以开始发送了。在高时延网络条件下，这样做可以降低网络的环回时间，提高性能。

对管道化连接有几条限制：  
- 如果 HTTP 客户端无法确认连接是持久的，就不应该使用管道。
- 必须按照与请求相同的顺序回送 HTTP 响应。HTTP 报文中没有序列号标签，因此如果收到的响应失序了，就没办法将其与请求匹配起来了。
- HTTP 客户端必须做好连接会在任意时刻关闭的准备，还要准备好重发所有未完成的管道化请求。如果客户端打开一条持久连接，并立即发出了10条请求，服务器处理了5条之后关闭了连接。。剩下的 5 条请求会失败，所以客户端必须能够应对这些过早关闭连接的情况，重新发出这些请求。

- HTTP 客户端不应该用管道化的方式发送会产生副作用的请求（比如 POST） 。总之，出错的时候，管道化方式会阻碍客户端了解服务器执行的是一系列管道化请求中的哪一些。由于无法安全地重试 POST 这样的非幂等请求，所以出错时，就存在某些方法永远不会被执行的风险。

# 4.7 关闭连接的奥秘

## 4.7.1 “任意”解除连接
所有 HTTP 客户端、服务器或代理都可以在任意时刻关闭一条 TCP 传输连接。通常会在一条报文结束时关闭连接

## 4.7.2 Content-Length及截尾操作
每条 HTTP 响应都应该有精确的 Content-Length 首部，用以描述响应主体的尺寸。

> 一些老的 HTTP 服务器会省略 Content-Length 首部，或者包含错误的长度指示，这样就要依赖服务器发出的连接关闭来说明数据的真实末尾。

客户端或代理收到一条随连接关闭而结束的HTTP响应，且实际传输的实体长度与Content-Length 并不匹配（或没有 Content-Length）时，接收端就应该质疑长度的正确性。

如果接收端是个缓存代理，接收端就不应该缓存这条响应（以降低今后将潜在的错误报文混合起来的可能） 。代理应该将有问题的报文原封不动地转发出去，而不应该试图去“校正”Content-Length，以维护语义的透明性。

## 4.7.3 连接关闭容限、重试以及幂等性
如果在客户端执行事务的过程中，传输连接关闭了，那么，除非事务处理会带来一些副作用，否则客户端就应该重新打开连接，并重试一次。对管道化连接来说，这种情况更加严重一些。客户端可以将大量请求放入队列中排队，但源端服务器可以关闭连接，这样就会留下大量未处理的请求，需要重新调度。

副作用是很重要的问题。如果在发送出一些请求数据之后，收到返回结果之前，连接关闭了，客户端就无法百分之百地确定服务器端实际激活了多少事务。有些事务，比如 GET 一个静态的 HTML 页面，可以反复执行多次，也不会有什么变化。而其他一些事务，比如向一个在线书店 POST 一张订单，就不能重复执行，不然会有下多张订单的危险（这样的情况就属于副作用）。

> ： 除非服务器怀疑出现了客户端或网络故障，否则就不应该在请求的中间关闭连接。

如果一个事务，不管是执行一次还是很多次，得到的结果都相同，这个事务就是幂等的。实现者们可以认为 GET、HEAD、PUT、DELETE、TRACE 和 OPTIONS 方法都共享这一特性。

客户端不应该以管道化方式传送非幂等请求（比如 POST） 。否则，传输连接的过早终止就会造成一些不确定的后果。要发送一条非幂等请求，就需要等待来自前一条请求的响应状态。

## 4.7.4 正常关闭连接
略

将数据传送到已关闭连接时会产生“连接被对端重置”错误。重置信息会清空你的的输入缓冲区。当期最终去输入缓冲区读取数据时，会得到一个连接被对端重置的错误。

### 正常关闭
HTTP规范建议，当客户端或服务器突然要关闭一条连接时，应该“正常的关闭输出连接”shutdown(SHUT_WR),这是因为你可以知道以什么时候发送完，但是你不能确定对方什么时候发送完。当两端都告诉对方它们不会再发送任何数据（比如关闭输出信道）之后，连接就会被完全关闭，而不会有重置的危险。

